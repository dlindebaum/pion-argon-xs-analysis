{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets the notebook see custom code\n",
    "import sys\n",
    "sys.path.insert(1, '/users/st15719/dune/xs-ana/pi0-analysis/analysis/')\n",
    "# Lets a module be reloaded without restarting the whole kernel\n",
    "from importlib import reload\n",
    "# Basic imports\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_gnn as tfgnn\n",
    "from tensorflow_gnn.models import gat_v2\n",
    "from tensorflow import keras\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import timeit\n",
    "import dill\n",
    "# Custom code\n",
    "from python.gnn import DataPreparation, Models, bdt_classifier, Layers\n",
    "from python.analysis import EventSelection, Plots, vector, PairSelection, Master, PFOSelection, cross_section, CutOptimization\n",
    "from python.analysis import SelectionEvaluation as seval\n",
    "import apps.cex_analysis_input as cai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up plot styling\n",
    "plt_conf = Plots.PlotConfig()\n",
    "plt_conf.SHOW_PLOT = True\n",
    "plt_conf.SAVE_FOLDER = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProtoDUNE event classification with Graph Neural Networks (GNNs)\n",
    "This notebook serves as a overview of the mechanism developed for creating, training, sav-/load-ing, and evaluation of a GNN.\n",
    "\n",
    "Most of the behind the scene is hidden within 3 python modules, found under `python/gnn/`.\n",
    " - `DataPreparation` handles formating the input data - moving from ntuples to tensorflow graphs, and the saving and loading thereof\n",
    " - `Layers` contains layers that we can use to construct our GNN.\n",
    " - `Models` handles construction of models from Layers, training, and basic output plots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "This section concerns data generation. It should only need be run when you make changes to the properties in the graphs themselves.\n",
    "\n",
    "Current graphs can be found (on sc01) at `/storage/wx21978/gnn`.\n",
    "\n",
    "First, the events are loaded. This applies a basic set of beam selection cuts.\n",
    "\n",
    "Then, we load a pre-trained BDT to use these as inputs for the nodes of the graphs.\n",
    "\n",
    "The properties to use in the graph are listed, refering to sets of dictionaries defining the properties we can use.\n",
    "These can be seen at the top of the `DataPreparation` module.\n",
    "These are used to define the graph generation.\n",
    "\n",
    "The graphs are finally generated by calling `DataPreparation.generate_training_data` on teh dictionary of parameters constructed by the above step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the events (ntuples) from which to make the graphs\n",
    "#conf_file = \"/users/wx21978/projects/pion-phys/analyses/3GeV_both/config_3GeV_MC_data.json\"\n",
    "#conf_args = cross_section.ApplicationArguments.ResolveConfig(cross_section.LoadConfiguration(conf_file))\n",
    "#args_c = cai.args_to_dict(conf_args)\n",
    "#file_dict = conf_args.ntuple_files[\"mc\"][1]\n",
    "#evts = Master.Data(file_dict[\"file\"], nTuple_type=\"PDSPAnalyser\", target_momentum=file_dict[\"pmom\"])\n",
    "#set_1_evts_selected = cai.BeamPionSelection(evts, args_c, True)\n",
    "\n",
    "# load events from already existing graphs\n",
    "old_path_params = DataPreparation.create_filepath_dictionary(\"/storage/wx21978/gnn/mc_graphs/3GeV_MC_Set00_final_07-08-24/\")\n",
    "with open(old_path_params[\"dict_path\"], \"rb\") as f:\n",
    "    old_params_dict = dill.load(f)\n",
    "old_evts = old_params_dict[\"events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the BDT as in input for the node information\n",
    "'''\n",
    "bdt_props = [\n",
    "    \"track\", \"length\", \"nhits\",\n",
    "    \"dEdx_med\", \"dEdx_sum\", \"dEdx_var\", \"dEdx_start10%\", \"dEdx_end25%\",\n",
    "    \"chi2_pion_reduced\", \"chi2_proton_reduced\", \"chi2_muon_reduced\"]\n",
    "bdt_truths = [\"pion\", \"photon\", \"proton\"]\n",
    "\n",
    "bdt_holder = bdt_classifier.BDTPropertyGenerator(bdt_props, bdt_truths)\n",
    "\n",
    "bdt_holder.train(evts)\n",
    "\n",
    "bdt_holder.save(\"/path/to/bdt/bdt_pfo.dll\")\n",
    "'''\n",
    "loaded_bdt = bdt_classifier.loadBDT(\"/storage/wx21978/gnn/bdt_pfo.dll\")\n",
    "\n",
    "bdt_classify_definitions = DataPreparation.default_kinematic_definitions\n",
    "def get_bdt_output(index, bdt):\n",
    "    def func(events):\n",
    "        n_pfos = ak.count(events.recoParticles.number, axis=1)\n",
    "        predictions = bdt.predict_proba(events)[:, index]\n",
    "        return ak.unflatten(predictions, n_pfos)\n",
    "    return func\n",
    "\n",
    "bdt_classify_definitions[\"bdt_pion\"] = get_bdt_output(0, loaded_bdt)\n",
    "bdt_classify_definitions[\"bdt_photon\"] = get_bdt_output(1, loaded_bdt)\n",
    "bdt_classify_definitions[\"bdt_proton\"] = get_bdt_output(2, loaded_bdt)\n",
    "bdt_classify_definitions[\"bdt_other\"] = get_bdt_output(3, loaded_bdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the information contained in the graph\n",
    "classifications = [ # These are the regions to use. Refer to DataPreparation.default_classification_definitions\n",
    "    \"absorption\", \"charge_exchange\",\n",
    "    \"all_pion_production\"]\n",
    "kinematic_props = [ # Information on each PFO\n",
    "    \"bdt_pion\", \"bdt_photon\", \"bdt_proton\", \"bdt_other\", # From BDT\n",
    "    \"track\", \"n_hits\"] # From ntuples (DataPreparation.default_kinematic_definitions)\n",
    "# Information between lines, see DataPreparation.default_geometric_definitions\n",
    "geom_props = [\"separation_shower_shower\", \"closest_approach_shower_shower\", \"impact_parameter_shower\"]\n",
    "# Information on beam connections, see DataPreparation.default_beam_definitions\n",
    "beam_props = [\"impact\", \"separation\"]\n",
    "\n",
    "graph_path = \"/users/st15719/dune/xs-ana/3GeV_MC_Set00_3cat\"\n",
    "\n",
    "data_bdt_params = DataPreparation.create_parameter_dictionary(\n",
    "    graph_path,\n",
    "    old_evts,\n",
    "    classifications, kinematic_props, geom_props, beam_connections=beam_props,\n",
    "    # Use the bdt_classify_definitions defined in above cell\n",
    "    kinematic_definitions=bdt_classify_definitions, norm_kinematics=True,\n",
    "    geometric_definitions=DataPreparation.default_geometric_definitions,\n",
    "    beam_definitions=DataPreparation.default_beam_definitions,\n",
    "    truth_info=True, # Include truth information (must be false when making data graphs)\n",
    "    data_type=\"reco\") # Reconstruction tier (no truth information), can be \"data\", \"bt\", or \"mc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N.B. takes 15-20 minutes\n",
    "DataPreparation.generate_training_data(data_bdt_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction\n",
    "Construction is handled by generating a list of Layers.\n",
    "Graph information flows linearly through the list of Layers, unless the Layer is passed with an strign argument, which identifies it as an output layer (referenced by the passed string).\n",
    "In this case, the state before the output layer continues through the network, and the result from the output layer can be sent to the output by passing a list of strings, corresponding to the names of the output layers.\n",
    "\n",
    "Additionaly, `LoopConstructor`s may be used to group together (and loop) sets of Layers.\n",
    "This can simply be used to generate repeating layers, or could be used to generate a more complex ouptut, which requires many layers which should not be passed further into the network.\n",
    "\n",
    "Details of each Layer (kwargs) should be looked up the `Layers`.\n",
    "A parameters dictionary stores any model-wide default parameters to use. This will be passed into each layer of the model, but can be overwritten if the Layer initialisation includes reference to one of these properties.\n",
    "The Layers should ignore any elements in the parameters dictionary which is unrelated to their layer.\n",
    "\n",
    "The model itself is only constrcted after the training is setup, since it uses file path of the hyper parameters to ensure the model is looking for graphs of the correct data.\n",
    "\n",
    "The main benefit of this method, is that it can save the full model configuration as a text file.\n",
    "This file can then be loaded (and even adjusted) to reload the model (with small tweaks).\n",
    "This new model can then have the weights which are save from the end of training imposed onto it.\n",
    "This works around and issue that was observed with trying to save GNNs which can output ragged tensor type data (i.e. per PFO losses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic model storage info\n",
    "\n",
    "# \"data\" for reco data only\n",
    "# \"bt\" for reco PFOs, but truth information\n",
    "# \"mc\" for truth PFOs and truth information\n",
    "data_type = \"data\"\n",
    "model_name = \"Model_data_PNA_1\"\n",
    "model_type=\"PNA\"\n",
    "\n",
    "#model_path = \"/users/st15719/dune/xs-ana/\" + model_name\n",
    "#print(f\"Model saved to:\\n{model_path}\")\n",
    "\n",
    "# we can load the paths dictionary here if we are starting from an already existing graph path, otherwise we use data_bdt_params dictionary from above\n",
    "#path_params = DataPreparation.create_filepath_dictionary(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default params for all layers\n",
    "model_params = {\n",
    "    \"node_dim\": 32,\n",
    "    \"beam_node_dim\": 64,\n",
    "    \"edge_dim\": 8,\n",
    "    \"beam_edge_dim\": 8,\n",
    "    # Dimensions for PNA message passing\n",
    "    \"message_dim\": 16,\n",
    "    \"beam_message_dim\": 32,\n",
    "    # Dimensions for GATv2 message passing.\n",
    "    # \"message_heads\": 4,\n",
    "    # \"message_channels\": 8,\n",
    "    # \"beam_message_heads\": 8,\n",
    "    # \"beam_message_channels\": 8,\n",
    "    # Other hyperparameters.\n",
    "    \"regularisation\": 8e-5,\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"activation\": \"leaky_relu\"\n",
    "}\n",
    "\n",
    "# Core message passing step of the netowrk (goes into a LoopContructor)\n",
    "model_message_passing = [\n",
    "    Layers.PFOUpdate(message_type=\"PNA\"),\n",
    "    Layers.NeighbourUpdate(final_step=False),\n",
    "    Layers.BeamCollection(message_type=\"PNA\"),\n",
    "    Layers.BeamConnectionUpdate(final_step=False)\n",
    "]\n",
    "\n",
    "# Core of the model, controling the flow, and update of information\n",
    "model_constructor = [\n",
    "    Layers.Setup(),\n",
    "    Models.create_normaliser_from_data(data_bdt_params),\n",
    "    Layers.InitialState(pfo_hidden=(128, 1),\n",
    "                        dropout_rate=0.3,# activation=\"leaky_relu\",\n",
    "                        neighbours_hidden=None,\n",
    "                        beam_connections_hidden=None),\n",
    "    # Layer is an output if given positional argument\n",
    "    Layers.ReadoutClassifyNode(\"pion_id\", which_nodes=\"pfo\"),\n",
    "    Layers.ReadoutClassifyNode(\"photon_id\", which_nodes=\"pfo\"),\n",
    "    Layers.BeamCollection(message_type=\"PNA\", next_state=\"concat\"),\n",
    "    # Loop constructor creates a sub loop\n",
    "    Layers.LoopConstructor(model_message_passing,\n",
    "                           loops=2),\n",
    "    Layers.ReadoutClassifyEdge(\"pi0_id\", which_edges=\"neighbours\"),\n",
    "    Layers.ReadoutNode(which_nodes=\"beam\"),\n",
    "    Layers.Dense(depth=64, n_layers=1),\n",
    "    Layers.Dense(\"pion_count\", depth=1, name=\"pion_count\", activation=\"relu\", regularisation=None),\n",
    "    Layers.Dense(\"pi0_count\", depth=1, name=\"pi0_count\", activation=\"relu\", regularisation=None),\n",
    "    Layers.Dense(depth=64, n_layers=2, dropout_rate=0.3),\n",
    "    Layers.Dense(\"reco_classification\", depth=3, name=\"reco_classifier\"),\n",
    "    Layers.Dense(\"classifier\", depth=3, activation=\"linear\", name=\"classifier\")\n",
    "]\n",
    "\n",
    "# References to labelled output Layers, controls the ordering of the outputs.\n",
    "model_outputs = [\"classifier\",\n",
    "                 \"pion_count\", \"pi0_count\",\n",
    "                 \"pion_id\", \"photon_id\", \"pi0_id\",\n",
    "                 \"reco_classification\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Each output of the model requires a loss to be trained against.\n",
    "These are specified in the first cell, along with metrics to let us track the performance of each loss.\n",
    "We also must specify the truth that it is aiming for, and some callbacks to assist training.\n",
    "\n",
    "A dictionary of hyper parameters is then generated to store information about the training process.\n",
    "This includes:\n",
    " - Paths/losses/metric/callbacks all previously defined\n",
    " - Basic learning rate (overwritten by the lr_scheduler callback)\n",
    " - Maximum number of epochs to train for (should be stopped by the early stopping callbakc before this)\n",
    " - Number of batches per epoch (654 calculated as the number of training events divided by batch size of 32)\n",
    " - The extra losses the model uses. See notes below.\n",
    " - Loss weights, relative importance of _all_ losses (see notes).\n",
    " - Path to the data.\n",
    " - Batch size.\n",
    " - How manyof the next events from which to randomly sample each batch as shuffling.\n",
    "\n",
    "After this is constructed, data and model are easily generated from the hyper params, and the model construction defined above.\n",
    "\n",
    "#### Extra losses\n",
    "\n",
    "The main classification loss is assumed to be always used, and set as the first model output.\n",
    "Any additional losses must be specified to the hyper parameters.\n",
    "These are given as a list of strings under the kwarg `extra_losses`,.\n",
    "These strings should reference and option in the `DataPreparation._make_decode_func` function.\n",
    "Any additional losses to be added must also be defined in thie function.\n",
    "\n",
    "The losses are then given a relative scaling (how big a graph update it causes) passed as a list of numbers in the `loss_weights` kwarg.\n",
    "Note that this list includes the base classifier (i.e. should have the same length as `model_outputs`), but the `extra_losses` list doesn't include the base, and this should have one# fewer element.\n",
    "\n",
    "#### Initial experiementation\n",
    "I am curious as to the performance of the GNN in terms model outputs if the `reco_class` is considered the more importance calssificaiton than the main classifier. You should be able to achieve this be editing the loss weights.\n",
    "\n",
    "Then, another thing of interest would be to not use logits in the final output. this will requrie a small tweak to the model construction as well.\n",
    "Logits are the raw GNN outputs, but it can be nicer to instead normalise the outputs as a unit normalised vector, with values betwen [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multiple loss parameters\"\"\"\n",
    "# Need one loss per model ouptut.\n",
    "# See https://www.tensorflow.org/api_docs/python/tf/keras/losses for an overview of losses\n",
    "loss = [tf.keras.losses.CategoricalFocalCrossentropy(gamma=3., alpha = [0.21, 0.48, 0.10], from_logits=True),\n",
    "        # tf.keras.losses.CategoricalFocalCrossentropy(gamma=3., alpha = [0.27, 0.48, 0.16, 0.09], from_logits=True),\n",
    "        # tf.keras.losses.CategoricalFocalCrossentropy(alpha = [0.275, 0.5, 0.2625, 0.1875], from_logits=True),\n",
    "        tf.keras.losses.MeanSquaredError(name=\"mse_pions\"),\n",
    "        tf.keras.losses.MeanSquaredError(name=\"mse_pi0s\"),\n",
    "        tf.keras.losses.BinaryFocalCrossentropy(name=\"pion_pfo_binary_focal_ce\", from_logits=True),\n",
    "        tf.keras.losses.BinaryFocalCrossentropy(name=\"photon_pfo_binary_focal_ce\", from_logits=True),\n",
    "        tf.keras.losses.BinaryFocalCrossentropy(name=\"pi0_edge_binary_focal_ce\", from_logits=True),\n",
    "        tf.keras.losses.CategoricalFocalCrossentropy(from_logits=True)]\n",
    "\n",
    "\n",
    "# See https://www.tensorflow.org/api_docs/python/tf/keras/metrics for an overview of metrics\n",
    "class_metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy_class\"),\n",
    "                 tf.keras.metrics.AUC(num_thresholds=100, from_logits=True, name=\"auc_class\"),\n",
    "                 tf.keras.metrics.F1Score(name=\"f1_score_class\"),\n",
    "                 tf.keras.metrics.F1Score(average=\"weighted\", name=\"f1_score_weighted_class\"),\n",
    "                 tf.keras.metrics.Precision(top_k=1, name=\"purity_class\"),\n",
    "                 tf.keras.metrics.Recall(top_k=1, name=\"efficiency_class\")]\n",
    "\n",
    "reco_class_metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy_reco_class\"),\n",
    "                      tf.keras.metrics.AUC(num_thresholds=100, from_logits=True, name=\"auc_reco_class\"),\n",
    "                      tf.keras.metrics.F1Score(name=\"f1_score_reco_class\"),\n",
    "                      tf.keras.metrics.F1Score(average=\"weighted\", name=\"f1_score_weighted_reco_class\"),\n",
    "                      tf.keras.metrics.Precision(top_k=1, name=\"purity_reco_class\"),\n",
    "                      tf.keras.metrics.Recall(top_k=1, name=\"efficiency_reco_class\")]\n",
    "\n",
    "regr_metrics_pion = [tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error_pion\"),\n",
    "                     tf.keras.metrics.R2Score(name=\"r^2_score_pion\")]\n",
    "regr_metrics_pi0 = [tf.keras.metrics.MeanAbsoluteError(name=\"mean_absolute_error_pi0\"),\n",
    "                    tf.keras.metrics.R2Score(name=\"r^2_score_pi0\")]\n",
    "\n",
    "# Generates a new list of Accruacy, Precision (purity) and Recal (efficiency) metrics\n",
    "def get_class_metric_list(id, base_name=\"pi0_id\", from_logits=True):\n",
    "     thresh = 0. if from_logits else 0.5\n",
    "     return [tf.keras.metrics.BinaryAccuracy(threshold=thresh, name=f\"binary_accuracy_{base_name}_{id}\"),\n",
    "                     tf.keras.metrics.Precision(thresholds=thresh, name=f\"purity_{base_name}_{id}\"),\n",
    "                     tf.keras.metrics.Recall(thresholds=thresh, name=f\"efficiency_{base_name}_{id}\")]\n",
    "\n",
    "metrics = [\n",
    "     class_metrics,\n",
    "     regr_metrics_pion, regr_metrics_pi0,\n",
    "     get_class_metric_list(\"pion\", \"pfo_id\"), get_class_metric_list(\"photon\", \"pfo_id\"),\n",
    "     get_class_metric_list(\"direct\"),\n",
    "     reco_class_metrics]\n",
    "\n",
    "# Manually specifies the learning rate (size of updates) as a function of the number of training steps performed\n",
    "def lr_scheduling(epoch):\n",
    "    if epoch < 8:\n",
    "        return 1e-3\n",
    "    if epoch > 20:\n",
    "        return 5e-5\n",
    "    # if (epoch > 148) and (epoch < 200):\n",
    "    #     return 1e-3\n",
    "    return 1e-4\n",
    "\n",
    "# Callbacks can adjust training parameters during the training process.\n",
    "# In this case, this is to stop when the model stopping improving,\n",
    "#   and adjusting the larning rate as a function of epoch\n",
    "callbacks=[\n",
    "     tf.keras.callbacks.EarlyStopping(\n",
    "          min_delta=1e-4, patience=24, restore_best_weights=True, start_from_epoch=16, verbose=1,\n",
    "          monitor=\"val_loss\"),\n",
    "     tf.keras.callbacks.LearningRateScheduler(lr_scheduling)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data, multiple loss version\"\"\"\n",
    "hyper_params = Models.generate_hyper_params(\n",
    "    graph_path,\n",
    "    loss,\n",
    "    metrics,\n",
    "    callbacks,#[0],\n",
    "    1e-4,\n",
    "    2048,\n",
    "    654,\n",
    "    extra_losses = [\"bt_pions\", \"bt_pi0s\", \"pion\", \"photon\", \"true_pi0\", \"reco_class\"],\n",
    "    loss_weights=[1,    5e-3  ,    1e-3  ,  1e-2 ,   1e-4  ,     1e-4  ,     1e-6    ],\n",
    "    # extra_losses = [\"pion\", \"photon\", \"true_pi0\", \"reco_class\"],\n",
    "    # loss_weights=[1, 1e-2 ,   1e-3  ,     1e-3  ,     1e-5    ],\n",
    "    # class_weights={0: 1.2, 1: 3., 2:1., 3:1.},\n",
    "    data_folder=data_bdt_params[\"folder_path\"],\n",
    "    training_batch=32,\n",
    "    training_shuffle=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_batched, val_ds_batched = Models.load_data_from_hyper_params(hyper_params)\n",
    "model = Models.construct_model(hyper_params,\n",
    "                               model_constructor,\n",
    "                               model_params,\n",
    "                               model_outputs,\n",
    "                               model_type=model_type)\n",
    "'''\n",
    "def compile_and_train(\n",
    "        model,\n",
    "        hyper_params,\n",
    "        batched_train,\n",
    "        batched_val,\n",
    "        print_summary=True,\n",
    "        partial_train=False):\n",
    "    \"\"\"\n",
    "Compiles and trains a TensorFlow Keras model based on the provided\n",
    "hyper parameters and datasets.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "model : tf.keras.Model\n",
    "    The TensorFlow Keras model to be compiled and trained.\n",
    "hyper_params : dict\n",
    "    Dictionary containing hyper-parameters for compiling and training\n",
    "    the model, as generated by `Models.generate_hyper_params`.\n",
    "batched_train : tf.data.Dataset\n",
    "    Batched training dataset.\n",
    "batched_val : tf.data.Dataset\n",
    "    Batched validation dataset.\n",
    "print_summary : bool, optional\n",
    "    Whether to print the model summary. Default is True.\n",
    "partial_train : bool, optional\n",
    "    If True, save the weights as \"_partial\" at the end of training.\n",
    "    This can be used as a training checkpoint to load later.  Intent is\n",
    "    to allow changing loss weightings partly through the process.\n",
    "    Default is False.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "history : tf.keras.callbacks.History\n",
    "    A record of training loss values and metrics values at successive\n",
    "    epochs, as well as validation loss values and validation metrics\n",
    "    values.\n",
    "\"\"\"\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate=hyper_params[\"learning_rate\"]),\n",
    "        loss=hyper_params[\"loss\"],\n",
    "        loss_weights = hyper_params[\"loss_weights\"],\n",
    "        metrics=hyper_params[\"metrics\"])\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    history = model.fit(batched_train,\n",
    "                        class_weight=hyper_params[\"class_weights\"],\n",
    "                        steps_per_epoch=hyper_params[\"steps_per_epoch\"],\n",
    "                        epochs=hyper_params[\"epochs\"],\n",
    "                        callbacks=hyper_params[\"callbacks\"],\n",
    "                        validation_data=batched_val)\n",
    "    # This save only saves a model which can do inference, it doesn't\n",
    "    #   have full (i.e. fitting) functionality\n",
    "    # model.export(hyper_params[\"model_path\"])\n",
    "    # Saving not currently working - need to use strings as dftype formatting?\n",
    "    if not partial_train:\n",
    "        model.save_weights(hyper_params[\"weights_path\"])\n",
    "        model.save(hyper_params[\"model_path\"], save_format=\"tf\", overwrite=False)\n",
    "    else:\n",
    "        partial_weights_path = hyper_params[\"weights_path\"] + \"_partial\"\n",
    "        model.save_weights(partial_weights_path)\n",
    "    with open(hyper_params[\"history_path\"], \"wb\") as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    return history\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "This is where the model actually trains.\n",
    "For implementing an adversarial training, a new function, similar to `Models.compile_and_train`, will need to be created.\n",
    "\n",
    "This will use the custom adversial training steps, and likely take in two models, one as the GNn, one as the adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Models.compile_and_train(model, hyper_params, train_ds_batched, val_ds_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes simple plots of all available metrics.\n",
    "Models.simple_plot_history(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and evaluating models\n",
    "As hinted the the Construction section, the models a first created using a model params text file.\n",
    "The train weights are them imposed onto this.\n",
    "\n",
    "Here, we load the model before evaluation to confirm it has saved properly (whilst you still have a copy in memory in case it does go wrong!), and then plot some information about performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all current records, excluding the training MC file.\n",
    "data_path_params = [\n",
    "    DataPreparation.create_filepath_dictionary(\"/storage/wx21978/gnn/data_graphs/3GeV_data_Set00_final_07-08-24/\"),\n",
    "    DataPreparation.create_filepath_dictionary(\"/storage/wx21978/gnn/data_graphs/3GeV_data_Set01_final_07-08-24/\"),\n",
    "    DataPreparation.create_filepath_dictionary(\"/storage/wx21978/gnn/data_graphs/3GeV_data_Set02_final_07-08-24/\"),\n",
    "    DataPreparation.create_filepath_dictionary(\"/storage/wx21978/gnn/data_graphs/3GeV_data_Set03_final_07-08-24/\")]\n",
    "\n",
    "mc_path_params = [\n",
    "    DataPreparation.create_filepath_dictionary(\"/storage/wx21978/gnn/mc_graphs/3GeV_MC_Set01_final_07-08-24\"),\n",
    "    DataPreparation.create_filepath_dictionary(\"/storage/wx21978/gnn/mc_graphs/3GeV_MC_Set02_final_07-08-24\")]\n",
    "\n",
    "which_path_params = mc_path_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to specify a model if you haven't trained one, else comment this out\n",
    "model_path = \"/storage/wx21978/gnn/models/Model_PNA_logits\"\n",
    "loaded_model = Models.load_model_from_file(model_path, new_data_folder=which_path_params[\"folder_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This chacks the main classification\n",
    "true_pred, true_truth = Models.evaluate_model(\n",
    "    loaded_model,\n",
    "    which_path_params,\n",
    "    classification_labels=[\"Abs.\", \"CEx.\", \"1 pi\", \"Multi.\"],\n",
    "    plot_config=plt_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks at the distribution of scores\n",
    "Models.total_score_dist(\n",
    "    loaded_model, which_path_params,\n",
    "    plt_conf, classification_labels=[\"Abs.\", \"CEx.\", \"1 pi\", \"Multi.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These look at the additional PF classification type losses\n",
    "Models.plot_binary_extra_loss_dist(\n",
    "    loaded_model,\n",
    "    \"pion\", 3,\n",
    "    which_path_params, plt_conf)\n",
    "Models.plot_binary_extra_loss_dist(\n",
    "    loaded_model,\n",
    "    \"photon\", 4,\n",
    "    which_path_params, plt_conf)\n",
    "Models.plot_binary_extra_loss_dist(\n",
    "    loaded_model,\n",
    "    \"true_pi0\", 5,#3,\n",
    "    which_path_params, plt_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks at how many pi0s/pions the model thinks are included in the event\n",
    "#   \"bt\" comapres to preconstructed object counts, \"mc\" compares to true object counts.\n",
    "which_true = \"bt\" # \"mc\"\n",
    "Models.plot_regression_extra_loss_dist(\n",
    "    loaded_model,\n",
    "    f\"{which_true}_pions\", 1,\n",
    "    which_path_params, plt_conf)\n",
    "Models.plot_regression_extra_loss_dist(\n",
    "    loaded_model,\n",
    "    f\"{which_true}_pi0s\", 2,\n",
    "    which_path_params, plt_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This looks at the reconstructed classification (based on counting reconstructed PFOs not true PFOs)\n",
    "# Also compares the reconstructed vs. true performance\n",
    "reco_pred, reco_truth = Models.plot_confusion_extra_loss(\n",
    "    loaded_model,\n",
    "    \"reco_class\", -1,\n",
    "    which_path_params,\n",
    "    classification_labels=[\"Abs.\", \"CEx.\", \"1 pi\", \"Multi.\"],\n",
    "    plot_config=plt_conf)\n",
    "    \n",
    "Models.plot_confusion_main_vs_reco_loss(\n",
    "    loaded_model,\n",
    "    \"reco_class\", -1,\n",
    "    which_path_params,\n",
    "    classification_labels=[\"Abs.\", \"CEx.\", \"1 pi\", \"Multi.\"],\n",
    "    plot_config=plt_conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xs-ana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
